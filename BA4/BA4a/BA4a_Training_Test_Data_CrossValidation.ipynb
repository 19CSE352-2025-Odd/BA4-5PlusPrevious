{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling\n",
    "# Demonstrate data splitting - Training-Validation-Testing - Cross Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a csv file to a dataFrame WestRox\n",
    "housing_df = pd.read_csv('https://raw.githubusercontent.com/reisanar/datasets/master/WestRoxbury.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL VALUE</th>\n",
       "      <th>TAX</th>\n",
       "      <th>LOT SQFT</th>\n",
       "      <th>YR BUILT</th>\n",
       "      <th>GROSS AREA</th>\n",
       "      <th>LIVING AREA</th>\n",
       "      <th>FLOORS</th>\n",
       "      <th>ROOMS</th>\n",
       "      <th>BEDROOMS</th>\n",
       "      <th>FULL BATH</th>\n",
       "      <th>HALF BATH</th>\n",
       "      <th>KITCHEN</th>\n",
       "      <th>FIREPLACE</th>\n",
       "      <th>REMODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344.2</td>\n",
       "      <td>4330</td>\n",
       "      <td>9965</td>\n",
       "      <td>1880</td>\n",
       "      <td>2436</td>\n",
       "      <td>1352</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>412.6</td>\n",
       "      <td>5190</td>\n",
       "      <td>6590</td>\n",
       "      <td>1945</td>\n",
       "      <td>3108</td>\n",
       "      <td>1976</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Recent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOTAL VALUE    TAX  LOT SQFT   YR BUILT  GROSS AREA   LIVING AREA  FLOORS   \\\n",
       "0         344.2  4330       9965      1880         2436         1352      2.0   \n",
       "1         412.6  5190       6590      1945         3108         1976      2.0   \n",
       "\n",
       "   ROOMS  BEDROOMS   FULL BATH  HALF BATH  KITCHEN  FIREPLACE REMODEL  \n",
       "0      6          3          1          1        1          0    None  \n",
       "1     10          4          2          1        1          0  Recent  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recap steps or jump directly to \"Predictive Power and Overfitting\" code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.REMODEL.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.columns = [s.strip().replace(\" \" , \"_\") for s in housing_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.REMODEL = housing_df.REMODEL.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['None', 'Old', 'Recent'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.REMODEL.cat.categories # Show number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['None', 'Old', 'Recent'], ordered=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.REMODEL.dtype # Check type of converted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dummy Variables in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use drop_first=True to drop the first dummy variable (There are 3 ie 'None', 'Old', 'Recent')\n",
    "housing_df = pd.get_dummies(housing_df, prefix_sep=\"_\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TOTAL_VALUE', 'TAX', 'LOT_SQFT', 'YR_BUILT', 'GROSS_AREA',\n",
       "       'LIVING_AREA', 'FLOORS', 'ROOMS', 'BEDROOMS', 'FULL_BATH', 'HALF_BATH',\n",
       "       'KITCHEN', 'FIREPLACE', 'REMODEL_Old', 'REMODEL_Recent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REMODEL_Old</th>\n",
       "      <th>REMODEL_Recent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   REMODEL_Old  REMODEL_Recent\n",
       "0            0               0\n",
       "1            0               1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.loc[:, \"REMODEL_Old\":\"REMODEL_Recent\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalizing (Standardizing) and Rescaling Data. This operation is also sometimes called standardizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seen last class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.5 Predictive Power and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In supervised learning, a key question presents itself: How well will our prediction or\n",
    "classification model perform when we apply it to new data? We are particularly interested\n",
    "in comparing the performance of various models so that we can choose the one we think\n",
    "will do the best when it is implemented in practice. A key concept is to make sure that our\n",
    "chosen model generalizes beyond the dataset that we have at hand. To assure\n",
    "generalization, we use the concept of data partitioning and try to avoid overfitting. These\n",
    "two important concepts are described next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfitting ---Overfitting: This function fits the data with no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Somewhat surprisingly, even if we know for a fact that a higher-degree curve is the\n",
    "appropriate model, if the model-fitting dataset is not large enough, a lower-degree\n",
    "function (that is not as likely to fit the noise) is likely to perform better in terms of\n",
    "predicting new values. Overfitting can also result from the application of many different\n",
    "models, from which the best performing model is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creation and Use of Data Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Partition\n",
    "The training partition, typically the largest partition, contains the data used to build the\n",
    "various models we are examining. The same training partition is generally used to\n",
    "develop multiple models.\n",
    "Validation Partition\n",
    "The validation partition (sometimes called the test partition) is used to assess the\n",
    "predictive performance of each model so that you can compare models and choose the\n",
    "best one. In some algorithms (e.g., classification and regression trees, k-nearest\n",
    "neighbors), the validation partition may be used in an automated fashion to tune and\n",
    "improve the model.\n",
    "Test Partition\n",
    "The test partition (sometimes called the holdout or evaluation partition) is used to assess\n",
    "the performance of the chosen model with new data.\n",
    "Why have both a validation and a test partition? When we use the validation data to\n",
    "assess multiple models and then choose the model that performs best with the validation\n",
    "data, we again encounter another (lesser) facet of the overfitting problem—chance aspects\n",
    "of the validation data that happen to match the chosen model better than they match\n",
    "other models. In other words, by using the validation data to choose one of several\n",
    "models, the performance of the chosen model on the validation data will be overly\n",
    "optimistic.\n",
    "The random features of the validation data that enhance the apparent performance of the\n",
    "chosen model will probably not be present in new data to which the model is applied.\n",
    "Therefore, we may have overestimated the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData = train_test_split(housing_df, test_size=0.40,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is train_test_split?\n",
    "train_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: \n",
    "for training data and for testing data. With this function, you don't need to divide the dataset manually.\n",
    "By default, Sklearn train_test_split will make random partitions for the two subsets. \n",
    "However, you can also specify a random state for the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(X, y, train_size=0.*,test_size=0.*, random_state=*)\n",
    "X, y. The first parameter is the dataset you're selecting to use.\n",
    "train_size. This parameter sets the size of the training dataset. There are three options: None, which is the default, \n",
    "Int, which requires the exact number of samples, and float, which ranges from 0.1 to 1.0.\n",
    "test_size. This parameter specifies the size of the testing dataset. The default state suits the training size. \n",
    "It will be set to 0.25 if the training size is set to default.\n",
    "random_state. The default mode performs a random split using np.random. Alternatively, you can add an integer using an exact number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "trainData, validData = model_selection.train_test_split(housing_df, test_size=0.40,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training :  (3481, 15)\n",
      "Validation :  (2321, 15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training : \", trainData.shape)\n",
    "print(\"Validation : \", validData.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training (50; and then splitting validation 40% and test 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training (50\n",
    "trainData, temp = model_selection.train_test_split(housing_df, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData, testData = model_selection.train_test_split(temp, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training :  (2901, 15)\n",
      "Test :  (1161, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training : \", trainData.shape)\n",
    "print(\"Test : \", testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-Validation\n",
    "When the number of records in our sample is small, data partitioning might not be\n",
    "advisable as each partition will contain too few records for model building and\n",
    "performance evaluation. Furthermore, some data mining methods are sensitive to small\n",
    "changes in the training data, so that a different partitioning can lead to different results.\n",
    "An alternative to data partitioning is cross-validation, which is especially useful with\n",
    "small samples. Cross-validation, or k-fold cross-validation, is a procedure that starts with\n",
    "partitioning the data into “folds,” or non-overlapping subsamples. Often we choose k = 5\n",
    "folds, meaning that the data are randomly partitioned into five equal parts, where each\n",
    "fold has 20% of the observations. A model is then fit k times. Each time, one of the folds\n",
    "is used as the validation set and the remaining k − 1 folds serve as the training set. The\n",
    "result is that each fold is used once as the validation set, thereby producing predictions\n",
    "for every observation in the dataset. We can then combine the model’s predictions on\n",
    "each of the k validation sets in order to evaluate the overall performance of the model. In\n",
    "Python, cross-validation is achieved using the cross_val_score() or the more general\n",
    "cross_validate function, where argument cv determines the number of folds. Sometimes\n",
    "cross-validation is built into a data mining algorithm, with the results of the crossvalidation\n",
    "used for choosing the algorithm’s parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
