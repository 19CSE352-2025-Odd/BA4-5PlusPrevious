{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The pandas package offers several methods that assist in summarizing data. The\n",
    "DataFrame method describe() gives an overview of the entire set of variables in the data.\n",
    "The methods mean(), std(), min(), max(), median(), and len() are also very helpful for\n",
    "learning about the characteristics of each variable. First, they give us information about\n",
    "the scale and type of values that the variable takes. The min and max statistics can be\n",
    "used to detect extreme values that might be errors. The mean and median give a sense of\n",
    "the central values of that variable, and a large deviation between the two also indicates\n",
    "skew. The standard deviation gives a sense of how dispersed the data are (relative to the\n",
    "mean). Further options, such as the combination of .isnull().sum(), which gives the\n",
    "number of null values, can tell us about missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) incorporating domain knowledge to remove or combine categories, \n",
    "(2) using data summaries to detect information overlap between variables\n",
    "(and remove or combine redundant variables or categories), \n",
    "(3) using data conversion techniques such as converting categorical variables into numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that subsets of variables are highly correlated with each other.\n",
    "Including highly correlated variables in a classification or prediction model, or including\n",
    "variables that are unrelated to the outcome of interest, can lead to overfitting, and\n",
    "accuracy and reliability can suffer. A large number of variables also poses computational\n",
    "problems for some supervised as well as unsupervised algorithms (aside from questions\n",
    "of correlation). In model deployment, superfluous variables can increase costs due to the\n",
    "collection and processing of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df = pd.read_csv('https://raw.githubusercontent.com/reisanar/datasets/master/BostonHousing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Description of Variables in the Boston Housing Dataset\n",
    "CRIM Crime rate\n",
    "ZN Percentage of residential land zoned for lots over 25,000 ft2\n",
    "INDUS Percentage of land occupied by nonretail business\n",
    "CHAS Does tract bound Charles River? (= 1 if tract bounds river, = 0 otherwise)\n",
    "NOX Nitric oxide concentration (parts per 10 million)\n",
    "RM Average number of rooms per dwelling\n",
    "AGE Percentage of owner-occupied units built prior to 1940\n",
    "DIS Weighted distances to five Boston employment centers\n",
    "RAD Index of accessibility to radial highways\n",
    "TAX Full-value property tax rate per $10,000\n",
    "PTRATIO Pupil-to-teacher ratio by town\n",
    "LSTAT Percentage of lower status of the population\n",
    "MEDV Median value of owner-occupied homes in $1000s\n",
    "CAT.MEDV Is median value of owner-occupied homes in tract above $30,000\n",
    "(CAT.MEDV = 1) or not (CAT.MEDV = 0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df = bostonHousing_df.rename(columns={\"CAT. MEDV\": \"CAT_MEDV\"})\n",
    "bostonHousing_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df = bostonHousing_df.rename(columns={\"CAT. MEDV\": \"CAT_MEDV\"})\n",
    "bostonHousing_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean, standard deviation, min, max, median, length, and missing values of\n",
    "# CRIM\n",
    "print('Mean : ', bostonHousing_df.CRIM.mean())\n",
    "print('Std. dev : ', bostonHousing_df.CRIM.std())\n",
    "print('Min : ', bostonHousing_df.CRIM.min())\n",
    "print('Max : ', bostonHousing_df.CRIM.max())\n",
    "print('Median : ', bostonHousing_df.CRIM.median())\n",
    "print('Length : ', len(bostonHousing_df.CRIM))\n",
    "print('Number of missing values : ', bostonHousing_df.CRIM.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean, standard dev., min, max, median, length, and missing values for all\n",
    "# variables\n",
    "pd.DataFrame({'mean': bostonHousing_df.mean(),\n",
    "'sd': bostonHousing_df.std(),\n",
    "'min': bostonHousing_df.min(),\n",
    "'max': bostonHousing_df.max(),\n",
    "'median': bostonHousing_df.median(),\n",
    "'length': len(bostonHousing_df),\n",
    "'miss.val': bostonHousing_df.isnull().sum(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize relationships between two or more variables. For numerical\n",
    "variables, we can compute a complete matrix of correlations between each pair of\n",
    "variables, using the pandas method corr().We see that most correlations are low and that many are\n",
    "negative. Recall also the visual display of a correlation matrix via a heatmap ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df.corr().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmaps: Visualizing Correlations and Missing Values\n",
    "A heatmap is a graphical display of numerical data where color is used to denote values.\n",
    "In a data mining context, heatmaps are especially useful for two purposes: for visualizing\n",
    "correlation tables and for visualizing missing values in the data. In both cases, the\n",
    "information is conveyed in a two-dimensional table. A correlation table for p variables has\n",
    "p rows and p columns. A data table contains p columns (variables) and n rows\n",
    "(observations). If the number of rows is huge, then a subset can be used. In both cases, it\n",
    "is much easier and faster to scan the color-coding rather than the values. Note that\n",
    "heatmaps are useful when examining a large number of values, but they are not a\n",
    "replacement for more precise graphical display, such as bar charts, because color\n",
    "differences cannot be perceived accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple heatmap of correlations (without values)\n",
    "corr = bostonHousing_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the colormap to a divergent scale and fix the range of the colormap\n",
    "#sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1,vmax=1, cmap = 'RdBu')\n",
    "# Include information about values (example demonstrate how to control the size of  the plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11, 7)\n",
    "sns.heatmap(corr, annot=True, fmt='.1f', cmap='RdBu', center=0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregation and Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df.CHAS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for aggregating MEDV by CHAS and RM\n",
    "Create bins of size 1 for variable using the method pd.cut. By default, the method creates a categorical variable \n",
    "The argument labels=False determines integers instead, e.g. 6.\n",
    "bostonHousing_df[\"RM_bin\"] = pd.cut(bostonHousing_df.RM, range(0, 10), labels=False)\n",
    "Compute the average of MEDV by (binned) RM and CHAS. \n",
    "First group the data frame using the groupby method, \n",
    "then restrict the analysis to MEDV and determine mean for each group.\n",
    "bostonHousing_df.groupby([’RM_bin’, ’CHAS’])[’MEDV’].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " bostonHousing_df[\"RM_bin\"]= pd.cut(bostonHousing_df.RM, range(0, 10),labels=False) # range is bin range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df[\"RM_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonHousing_df.groupby([\"RM_bin\", \"CHAS\"])[\"MEDV\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is pivot_table() in the pandas package, that allows the creation of pivot tables by reshaping the data by the aggregating variables of our choice. For example, code below computes the average of MEDV by CHAS and RM and presents it as a pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(bostonHousing_df, values=\"MEDV\", index=[\"RM_bin\"], columns= [\"CHAS\"],aggfunc=np.mean, margins=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
